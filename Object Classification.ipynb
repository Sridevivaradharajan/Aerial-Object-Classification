{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13819190,"sourceType":"datasetVersion","datasetId":8800340}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\n\n# Deep Learning - TensorFlow/Keras\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten, Dense, \n                                     Dropout, BatchNormalization, \n                                     GlobalAveragePooling2D, Input)\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.applications import ResNet50, InceptionV3\n\n# Metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# %%\n# Set random seeds for reproducibility\nSEED = 42\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\nrandom.seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T15:45:39.806804Z","iopub.execute_input":"2025-11-21T15:45:39.807109Z","iopub.status.idle":"2025-11-21T15:46:09.848045Z","shell.execute_reply.started":"2025-11-21T15:45:39.807086Z","shell.execute_reply":"2025-11-21T15:46:09.846856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/object-classification'\nTRAIN_DIR = os.path.join(BASE_PATH, 'train')  # Changed to lowercase\nVALID_DIR = os.path.join(BASE_PATH, 'valid')  # Changed to lowercase\nTEST_DIR = os.path.join(BASE_PATH, 'test')    # Changed to lowercase\n\n# Class names\nCLASS_NAMES = ['bird', 'drone']\n\nprint(f\"Base Path: {BASE_PATH}\")\nprint(f\"Train Directory: {TRAIN_DIR}\")\nprint(f\"Valid Directory: {VALID_DIR}\")\nprint(f\"Test Directory: {TEST_DIR}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T16:07:44.191282Z","iopub.execute_input":"2025-11-21T16:07:44.191659Z","iopub.status.idle":"2025-11-21T16:07:44.199239Z","shell.execute_reply.started":"2025-11-21T16:07:44.191633Z","shell.execute_reply":"2025-11-21T16:07:44.197861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def count_images(base_path):\n    \"\"\"Count images in each class for all splits\"\"\"\n    data = []\n    for split in ['train', 'valid', 'test']:   # <-- FIXED lowercase\n        split_path = os.path.join(base_path, split)\n        if os.path.exists(split_path):\n            for cls in os.listdir(split_path):\n                cls_path = os.path.join(split_path, cls)\n                if os.path.isdir(cls_path):\n                    count = len([\n                        f for f in os.listdir(cls_path)\n                        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n                    ])\n                    data.append({\n                        'Split': split.upper(),  # Better for display\n                        'Class': cls,\n                        'Count': count\n                    })\n    return pd.DataFrame(data)\n\n\nBASE_PATH = \"/kaggle/input/object-classification\"\ndf_counts = count_images(BASE_PATH)\n\nprint(\"Dataset Distribution:\")\nprint(df_counts.pivot(index='Class', columns='Split', values='Count'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T16:09:13.305845Z","iopub.execute_input":"2025-11-21T16:09:13.306219Z","iopub.status.idle":"2025-11-21T16:09:13.656970Z","shell.execute_reply.started":"2025-11-21T16:09:13.306195Z","shell.execute_reply":"2025-11-21T16:09:13.655958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n\nfor idx, split in enumerate(['TRAIN', 'VALID', 'TEST']):\n    split_data = df_counts[df_counts['Split'] == split]\n    colors = ['#3498db', '#e74c3c']\n    \n    axes[idx].bar(split_data['Class'], split_data['Count'], color=colors)\n    axes[idx].set_title(f'{split} Set Distribution', fontsize=12, fontweight='bold')\n    axes[idx].set_xlabel('Class')\n    axes[idx].set_ylabel('Number of Images')\n\n    # ---- LABELS INSIDE BARS ----\n    for i, (cls, count) in enumerate(zip(split_data['Class'], split_data['Count'])):\n        axes[idx].text(\n            i, \n            count * 0.5,          # middle height\n            str(count),\n            ha='center',\n            va='center',\n            fontweight='bold',\n            color='white' if count > 20 else 'black'   # white text if bar is tall\n        )\n\nplt.tight_layout()\nplt.savefig('dataset_distribution.png', dpi=160, bbox_inches='tight')\nplt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = df_counts[df_counts['Split'] == 'TRAIN']\ntotal_train = train_data['Count'].sum()\nprint(\"\\nClass Imbalance Analysis (Training Set):\")\nfor _, row in train_data.iterrows():\n    percentage = (row['Count'] / total_train) * 100\n    print(f\"  {row['Class']}: {row['Count']} images ({percentage:.1f}%)\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize sample images from each class\ndef display_sample_images(data_dir, class_names, samples_per_class=5):\n    \"\"\"Display sample images from each class\"\"\"\n    fig, axes = plt.subplots(len(class_names), samples_per_class, \n                             figsize=(3*samples_per_class, 3*len(class_names)))\n    \n    for i, cls in enumerate(class_names):\n        cls_path = os.path.join(data_dir, cls)\n        images = os.listdir(cls_path)\n        sample_images = random.sample(images, min(samples_per_class, len(images)))\n        \n        for j, img_name in enumerate(sample_images):\n            img_path = os.path.join(cls_path, img_name)\n            img = Image.open(img_path)\n            axes[i, j].imshow(img)\n            axes[i, j].axis('off')\n            if j == 0:\n                axes[i, j].set_title(f'{cls.upper()}', fontsize=12, fontweight='bold')\n    \n    plt.suptitle('Sample Images from Training Set', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig('sample_images.png', dpi=150, bbox_inches='tight')\n    plt.show()\n\ndisplay_sample_images(TRAIN_DIR, CLASS_NAMES)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check image properties (size, channels)\ndef analyze_image_properties(data_dir, num_samples=50):\n    \"\"\"Analyze image dimensions and properties\"\"\"\n    widths, heights = [], []\n    \n    for cls in os.listdir(data_dir):\n        cls_path = os.path.join(data_dir, cls)\n        if os.path.isdir(cls_path):\n            images = os.listdir(cls_path)[:num_samples]\n            for img_name in images:\n                img_path = os.path.join(cls_path, img_name)\n                try:\n                    img = Image.open(img_path)\n                    widths.append(img.size[0])\n                    heights.append(img.size[1])\n                except:\n                    pass\n    \n    print(\"Image Dimension Statistics:\")\n    print(f\"  Width  - Min: {min(widths)}, Max: {max(widths)}, Mean: {np.mean(widths):.0f}\")\n    print(f\"  Height - Min: {min(heights)}, Max: {max(heights)}, Mean: {np.mean(heights):.0f}\")\n    \n    return widths, heights\n\nwidths, heights = analyze_image_properties(TRAIN_DIR)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configuration\nIMG_SIZE = (224, 224)  # Standard size for most pretrained models\nBATCH_SIZE = 32\nNUM_CLASSES = 2\n\nprint(f\"Image Size: {IMG_SIZE}\")\nprint(f\"Batch Size: {BATCH_SIZE}\")\nprint(f\"Number of Classes: {NUM_CLASSES}\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create ImageDataGenerators\n\n# Training data generator WITH augmentation\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,                    # Normalize pixel values to [0, 1]\n    rotation_range=30,                 # Random rotation\n    width_shift_range=0.2,             # Horizontal shift\n    height_shift_range=0.2,            # Vertical shift\n    shear_range=0.2,                   # Shear transformation\n    zoom_range=0.2,                    # Random zoom\n    horizontal_flip=True,              # Random horizontal flip\n    vertical_flip=True,                # Random vertical flip\n    brightness_range=[0.8, 1.2],       # Brightness adjustment\n    fill_mode='nearest'                # Fill mode for new pixels\n)\n\n# Validation & Test data generator - ONLY rescaling (no augmentation)\nval_test_datagen = ImageDataGenerator(rescale=1./255)\n\n# %%\n# Create data generators\ntrain_generator = train_datagen.flow_from_directory(\n    TRAIN_DIR,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    shuffle=True,\n    seed=SEED\n)\n\nvalid_generator = val_test_datagen.flow_from_directory(\n    VALID_DIR,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    shuffle=False\n)\n\ntest_generator = val_test_datagen.flow_from_directory(\n    TEST_DIR,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    shuffle=False\n)\n\n# %%\n# Print class indices\nprint(\"Class Indices:\", train_generator.class_indices)\nprint(f\"\\nTraining samples: {train_generator.samples}\")\nprint(f\"Validation samples: {valid_generator.samples}\")\nprint(f\"Test samples: {test_generator.samples}\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize augmented images\ndef visualize_augmentation(data_dir, class_name, num_augmented=6):\n    \"\"\"Visualize original vs augmented images\"\"\"\n    cls_path = os.path.join(data_dir, class_name)\n    img_name = random.choice(os.listdir(cls_path))\n    img_path = os.path.join(cls_path, img_name)\n    \n    # Load original image\n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=IMG_SIZE)\n    img_array = tf.keras.preprocessing.image.img_to_array(img)\n    img_array = img_array.reshape((1,) + img_array.shape)\n    \n    # Generate augmented images\n    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n    axes = axes.flatten()\n    \n    # Original image\n    axes[0].imshow(img)\n    axes[0].set_title('ORIGINAL', fontsize=10, fontweight='bold')\n    axes[0].axis('off')\n    \n    # Augmented images\n    aug_iter = train_datagen.flow(img_array, batch_size=1)\n    for i in range(1, 8):\n        aug_img = next(aug_iter)[0]\n        axes[i].imshow(aug_img)\n        axes[i].set_title(f'Augmented {i}', fontsize=10)\n        axes[i].axis('off')\n    \n    plt.suptitle(f'Data Augmentation Examples - {class_name.upper()}', \n                 fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig('augmentation_examples.png', dpi=150, bbox_inches='tight')\n    plt.show()\n\nvisualize_augmentation(TRAIN_DIR, 'bird')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_custom_cnn():\n    \"\"\"Build a custom CNN architecture from scratch\"\"\"\n    model = Sequential([\n        # Input Layer\n        Input(shape=(224, 224, 3)),\n        \n        # Convolutional Block 1\n        Conv2D(32, (3, 3), activation='relu', padding='same'),\n        BatchNormalization(),\n        Conv2D(32, (3, 3), activation='relu', padding='same'),\n        BatchNormalization(),\n        MaxPooling2D((2, 2)),\n        Dropout(0.25),\n        \n        # Convolutional Block 2\n        Conv2D(64, (3, 3), activation='relu', padding='same'),\n        BatchNormalization(),\n        Conv2D(64, (3, 3), activation='relu', padding='same'),\n        BatchNormalization(),\n        MaxPooling2D((2, 2)),\n        Dropout(0.25),\n        \n        # Convolutional Block 3\n        Conv2D(128, (3, 3), activation='relu', padding='same'),\n        BatchNormalization(),\n        Conv2D(128, (3, 3), activation='relu', padding='same'),\n        BatchNormalization(),\n        MaxPooling2D((2, 2)),\n        Dropout(0.25),\n        \n        # Convolutional Block 4\n        Conv2D(256, (3, 3), activation='relu', padding='same'),\n        BatchNormalization(),\n        Conv2D(256, (3, 3), activation='relu', padding='same'),\n        BatchNormalization(),\n        MaxPooling2D((2, 2)),\n        Dropout(0.25),\n        \n        # Fully Connected Layers\n        Flatten(),\n        Dense(512, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.5),\n        Dense(256, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.5),\n        \n        # Output Layer\n        Dense(1, activation='sigmoid')\n    ])\n    \n    return model\n\n# %%\n# Build and compile Custom CNN\ncustom_cnn = build_custom_cnn()\n\ncustom_cnn.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\n# Display model summary\nprint(\"=\" * 60)\nprint(\"CUSTOM CNN ARCHITECTURE\")\nprint(\"=\" * 60)\ncustom_cnn.summary()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_transfer_model(base_model_name='resnet50', trainable=False):\n    \n    # Select base model\n    if base_model_name == 'resnet50':\n        base_model = ResNet50(\n            weights='imagenet', \n            include_top=False, \n            input_shape=(224, 224, 3)\n        )\n    elif base_model_name == 'inceptionv3':\n        base_model = InceptionV3(\n            weights='imagenet', \n            include_top=False, \n            input_shape=(224, 224, 3)\n        )\n    else:\n        raise ValueError(f\"Unknown model: {base_model_name}\")\n    \n    # Freeze/Unfreeze base model\n    base_model.trainable = trainable\n    \n    # Build the model\n    inputs = Input(shape=(224, 224, 3))\n    x = base_model(inputs, training=False)\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(256, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(128, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    outputs = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs, outputs)\n    \n    return model, base_model\n\n# %%\n# Build Transfer Learning Models\n\n# ResNet50\nprint(\"Building ResNet50...\")\nresnet_model, resnet_base = build_transfer_model('resnet50')\nresnet_model.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\n# InceptionV3\nprint(\"Building InceptionV3...\")\ninceptionv3_model, inceptionv3_base = build_transfer_model('inceptionv3')\ninceptionv3_model.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nprint(\"\\n‚úÖ All transfer learning models built successfully!\")\n\n# %%\n# Display ResNet50 model summary\nprint(\"=\" * 60)\nprint(\"RESNET50 TRANSFER LEARNING ARCHITECTURE\")\nprint(\"=\" * 60)\nresnet_model.summary()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define callbacks\ndef get_callbacks(model_name):\n    \"\"\"Create callbacks for training\"\"\"\n    callbacks = [\n        EarlyStopping(\n            monitor='val_loss',\n            patience=10,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        ModelCheckpoint(\n            f'models/best_{model_name}.keras',\n            monitor='val_accuracy',\n            save_best_only=True,\n            verbose=1\n        ),\n        ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=5,\n            min_lr=1e-7,\n            verbose=1\n        )\n    ]\n    return callbacks\n\n# Create models directory\nos.makedirs('models', exist_ok=True)\n\nEPOCHS = 30\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train Custom CNN\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TRAINING CUSTOM CNN\")\nprint(\"=\" * 60)\n\nhistory_cnn = custom_cnn.fit(\n    train_generator,\n    validation_data=valid_generator,\n    epochs=EPOCHS,\n    callbacks=get_callbacks('custom_cnn'),\n    verbose=1\n)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%\n# Train ResNet50\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TRAINING RESNET50 (Feature Extraction)\")\nprint(\"=\" * 60)\n\nhistory_resnet = resnet_model.fit(\n    train_generator,\n    validation_data=valid_generator,\n    epochs=30,\n    callbacks=get_callbacks('resnet50'),\n    verbose=1\n)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train InceptionV3\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TRAINING INCEPTIONV3 (Feature Extraction)\")\nprint(\"=\" * 60)\n\nhistory_inceptionv3 = inceptionv3_model.fit(\n    train_generator,\n    validation_data=valid_generator,\n    epochs=30,\n    callbacks=get_callbacks('inceptionv3'),\n    verbose=1\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_training_history(history, model_name):\n    \"\"\"Plot training and validation metrics\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Accuracy\n    axes[0].plot(history.history['accuracy'], label='Train', linewidth=2)\n    axes[0].plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n    axes[0].set_title(f'{model_name} - Accuracy', fontsize=12, fontweight='bold')\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Accuracy')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n    \n    # Loss\n    axes[1].plot(history.history['loss'], label='Train', linewidth=2)\n    axes[1].plot(history.history['val_loss'], label='Validation', linewidth=2)\n    axes[1].set_title(f'{model_name} - Loss', fontsize=12, fontweight='bold')\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Loss')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(f'{model_name}_training_history.png', dpi=150, bbox_inches='tight')\n    plt.show()\n\n# Plot training histories\nplot_training_history(history_cnn, 'Custom_CNN')\nplot_training_history(history_resnet, 'ResNet50')\nplot_training_history(history_inceptionv3, 'InceptionV3')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, test_generator, model_name):\n    \"\"\"Comprehensive model evaluation\"\"\"\n    print(f\"\\n{'=' * 60}\")\n    print(f\"EVALUATING {model_name}\")\n    print(f\"{'=' * 60}\")\n    \n    # Reset generator\n    test_generator.reset()\n    \n    # Get predictions\n    y_pred_proba = model.predict(test_generator, verbose=1)\n    y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n    y_true = test_generator.classes\n    \n    # Test loss and accuracy\n    test_loss, test_acc = model.evaluate(test_generator, verbose=0)\n    \n    # Classification Report\n    print(\"\\nüìä Classification Report:\")\n    print(classification_report(y_true, y_pred, target_names=CLASS_NAMES))\n    \n    # Confusion Matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n                annot_kws={'size': 14})\n    plt.title(f'{model_name} - Confusion Matrix', fontsize=14, fontweight='bold')\n    plt.xlabel('Predicted', fontsize=12)\n    plt.ylabel('Actual', fontsize=12)\n    plt.tight_layout()\n    plt.savefig(f'{model_name}_confusion_matrix.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    return {\n        'model_name': model_name,\n        'test_accuracy': test_acc,\n        'test_loss': test_loss,\n        'y_pred': y_pred,\n        'y_true': y_true\n    }\n\n# %%\n# Evaluate all models\nresults = []\n\nresults.append(evaluate_model(custom_cnn, test_generator, 'Custom_CNN'))\nresults.append(evaluate_model(resnet_model, test_generator, 'ResNet50'))\nresults.append(evaluate_model(inceptionv3_model, test_generator, 'InceptionV3'))\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create comparison dataframe\ncomparison_df = pd.DataFrame([\n    {'Model': r['model_name'], \n     'Test Accuracy': r['test_accuracy'], \n     'Test Loss': r['test_loss']} \n    for r in results\n])\n\ncomparison_df = comparison_df.sort_values('Test Accuracy', ascending=False)\nprint(\"\\nüìä MODEL COMPARISON:\")\nprint(comparison_df.to_string(index=False))\n\n# %%\n# Visualize model comparison\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Accuracy comparison\ncolors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\nbars1 = axes[0].bar(comparison_df['Model'], comparison_df['Test Accuracy'], color=colors)\naxes[0].set_ylim(0, 1)\naxes[0].set_ylabel('Accuracy', fontsize=12)\naxes[0].set_title('Model Comparison - Test Accuracy', fontsize=14, fontweight='bold')\naxes[0].tick_params(axis='x', rotation=45)\n\nfor bar in bars1:\n    height = bar.get_height()\n    axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n                f'{height:.2%}', ha='center', va='bottom', fontweight='bold')\n\n# Loss comparison\nbars2 = axes[1].bar(comparison_df['Model'], comparison_df['Test Loss'], color=colors)\naxes[1].set_ylabel('Loss', fontsize=12)\naxes[1].set_title('Model Comparison - Test Loss', fontsize=14, fontweight='bold')\naxes[1].tick_params(axis='x', rotation=45)\n\nfor bar in bars2:\n    height = bar.get_height()\n    axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n                f'{height:.4f}', ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Find best model\nbest_model_info = comparison_df.iloc[0]\nprint(f\"\\nüèÜ BEST MODEL: {best_model_info['Model']}\")\nprint(f\"   Test Accuracy: {best_model_info['Test Accuracy']:.2%}\")\nprint(f\"   Test Loss: {best_model_info['Test Loss']:.4f}\")\n\n# Save all models\ncustom_cnn.save('models/custom_cnn_final.keras')\nresnet_model.save('models/resnet50_final.keras')\ninceptionv3_model.save('models/inceptionv3_final.keras')\n\nprint(\"‚úÖ All models saved to 'models/' directory\")\n\n# %%\n# Save the best model for Streamlit deployment\nbest_model_name = best_model_info['Model'].lower().replace('_', '')\n\n# Copy best model to deployment folder\nimport shutil\nos.makedirs('streamlit_app', exist_ok=True)\n\nif 'cnn' in best_model_name:\n    shutil.copy('models/custom_cnn_final.keras', 'streamlit_app/best_model.keras')\nelif 'resnet' in best_model_name:\n    shutil.copy('models/resnet50_final.keras', 'streamlit_app/best_model.keras')\nelif 'inception' in best_model_name:\n    shutil.copy('models/inceptionv3_final.keras', 'streamlit_app/best_model.keras')\n\nprint(f\"‚úÖ Best model ({best_model_info['Model']}) saved for Streamlit deployment\")\n","metadata":{},"outputs":[],"execution_count":null}]}
